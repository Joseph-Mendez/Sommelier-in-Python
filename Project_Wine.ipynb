{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                  \"Wine Predictor with Random Forest Regressor\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Joe Mendez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project we will use Machine Learning to build a predictive model that helps determine the quality of a certain wine or wines. When data about a wine is entered into the model, we will see if the wine is great, just ok, or not very good at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, let's get started! Now first we will import all the libraries and modules needed for this project. We will be using Python 2.7 for this model, and modules from libraries: scikit-learn, Numpy, and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first we will import scikit-learn, which is a great machine learning library for python\n",
    "import sklearn\n",
    "\n",
    "#next we will import the Numpy library. Numpy is great for numerical computations and for statistics when using python\n",
    "import numpy as np\n",
    "\n",
    "#Next we will import Pandas. Pandas is a library in Python that makes DataFrames easy to work with\n",
    "import pandas as pd\n",
    "\n",
    "#import train_test_split function from scikit-learn's module model_selection will help with... you guessed it: model selection!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import preprocessing model from scikit-learn for scaling, wrangling and transforming data during preprocessing\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will start to put together the tools we will use to train our model and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import Random Forest model from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#to help with cross-validation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#to evaluate the model's performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "#import module to save scikit-learn models. Joblib is similar to pickle, another popular python library \n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will load and view the wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the wine dataset from a remote url\n",
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url, sep=';')#read in dataset with \";\" accounted for as delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    }
   ],
   "source": [
    "#let's view the samples and features\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 1599 samples and 12 features(categories) in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's print out a summary of the data to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "#summary statistics\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a list of the features we will be working with in this dataset: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fixed acidity\n",
    "volatile acidity\n",
    "citric acid\n",
    "residual sugar\n",
    "chlorides\n",
    "free sulfur dioxide\n",
    "total sulfur dioxide\n",
    "density\n",
    "pH\n",
    "sulphates\n",
    "alcohol\n",
    "quality (target feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we remove our target feature \"quality\" from our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we split the data into a test set and a training set: Splitting into test and training sets at the beginning of your model is crucial to getting a realistic estimate of your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using train_test_split() from scikit-learn's model_selection module; we will utilize 20% of the data for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using a feature in scikit-learn called the Transformer API. The Transformer API allows you to \"fit\" a preprocessing step using the training data the same way you'd fit a model. This makes your final estimate of model performance more realistic, and it allows to insert your preprocessing steps into a cross-validation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the Transformer API to the training set(saving the means and standard deviations)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply transformer API to the training set\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16664562e-16  -3.05550043e-17  -8.47206937e-17  -2.22218213e-17\n",
      "   2.22218213e-17  -6.38877362e-17  -4.16659149e-18  -2.54439854e-15\n",
      "  -8.70817622e-16  -4.08325966e-16  -1.17220107e-15]\n"
     ]
    }
   ],
   "source": [
    "#print out mean of the training set \n",
    "print X_train_scaled.mean(axis=0)                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#print out standard deviation\n",
    "print X_train_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we do the same for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply transformer API to the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n"
     ]
    }
   ],
   "source": [
    "#print out the mean of the test set\n",
    "print X_test_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.02160495  1.00135689  0.97456598  0.91099054  0.86716698  0.94193125\n",
      "  1.03673213  1.03145119  0.95734849  0.83829505  1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "#print out the standard deviation of the test set\n",
    "print X_test_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline with preprocessing and modeling\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & Model Parameters. Within each decision tree, the computer can decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). So the actual branch locations are model parameters. However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. The examples of hyperparameters that the we must set are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__random_state': None, 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'standardscaler__with_mean': True, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__min_samples_leaf': 1, 'standardscaler__copy': True, 'randomforestregressor__warm_start': False, 'randomforestregressor__criterion': 'mse', 'randomforestregressor__n_jobs': 1, 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__oob_score': False, 'randomforestregressor__verbose': 0, 'randomforestregressor__min_impurity_split': 1e-07, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))], 'randomforestregressor__min_samples_split': 2, 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False), 'randomforestregressor__max_depth': None, 'standardscaler__with_std': True, 'randomforestregressor__max_features': 'auto', 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'randomforestregressor__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#List tuneable hyperparameters\n",
    "print pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting hyperparameters\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV performs cross-validation across the entire \"grid\"of hyperparameters. It takes in your model, the hyperparameters you want to tune, and the number of \"folds\" to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1], 'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scikit-learn cross-validation with pipeline\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, you can see the best set of parameters found using CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "#printing out the best hyperparameters\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is time to refit the model. GridSearchCV will automatically refit for you. To check if default refit is on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#check if auto-refit through GridSearchCV is on. \"True\" = on\n",
    "print clf.refit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is how we predict a new set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict new set of data\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With earlier imported metrics we can now evaluate our model's performance. Let's see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.468295659544\n"
     ]
    }
   ],
   "source": [
    "print r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3430946875\n"
     ]
    }
   ],
   "source": [
    "#print mean sq. error\n",
    "print mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you have a pipeline model to evaluate wines of all kinds! Let's make sure we save this model for future use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_regressor.pkl']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next time you want to use the model just use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    " \n",
    "# Predict data set using loaded model\n",
    "clf2.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
